{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4f7ff5",
   "metadata": {},
   "source": [
    "# Energy Forecasting with Deep Learning (Part 1)\n",
    "author: Andrew Mendez, andrew.mendez@hpe.com\n",
    "\n",
    "Version: 0.0.1\n",
    "\n",
    "Date: 3.14.23\n",
    "\n",
    "This tutorial introduces the process of training a deep learning model for time series forecasting, specifically forecasting energy demand. Time series forecasting is a crucial aspect of many domains, including energy management, where accurate predictions can lead to efficient energy use and help in decision-making processes.\n",
    "\n",
    "## Dataset\n",
    "The dataset used in this example contains various energy generation and consumption data. We focus on the 'generation fossil gas' feature to predict future energy demand.\n",
    "\n",
    "## Model\n",
    "We employ a Temporal Convolutional Network (TCN) model, known for its effectiveness in handling sequence data like time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f20b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.models import TCNModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.metrics import mse, rmse\n",
    "from darts.datasets import EnergyDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "\n",
    "# Helper function for model configuration\n",
    "def generate_torch_kwargs():\n",
    "    return {\n",
    "        'pl_trainer_kwargs': {\n",
    "            'accelerator': 'cpu',\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7809a",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The following steps prepare the dataset for training our TCN model.\n",
    "\n",
    "The dataset comprises 4 years of data on electrical consumption, generation, pricing, and weather for Spain. It includes detailed hourly data, allowing for fine-grained analysis and forecasting of energy demand and supply dynamics. More details about the dataset can be seen [here](https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the energy dataset\n",
    "\n",
    "df = pd.read_csv('data/energy_dataset.csv', parse_dates=['time'])\n",
    "df.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adec06a",
   "metadata": {},
   "source": [
    "## Time Series Transformation\n",
    "We transform the dataset to make it suitable for the TCN model. This includes normalizing the data and splitting it into training and validation sets.\n",
    "\n",
    "We focus on forecasting 'generation hydro run-of-river and poundage', which represents the hydroelectric generation in megawatts (MW). This feature is particularly interesting for forecasting due to its dependence on natural water flow and storage, making it sensitive to weather conditions and potentially more challenging to predict accurately compared to other generation methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_and_val_and_save(df):\n",
    "    '''\n",
    "    '''\n",
    "    # Aggregating by day to simplify the model's task, aiming to capture daily patterns\n",
    "    # in hydroelectric generation which might be influenced by daily weather changes\n",
    "    # and consumption patterns.\n",
    "    df_day_avg = df.resample('D').mean()\n",
    "    series = TimeSeries.from_dataframe(df_day_avg, value_cols=['generation hydro run-of-river and poundage'])\n",
    "\n",
    "    # Splitting the series into training and validation sets, with the split point chosen\n",
    "    # to ensure the model is trained on a substantial historical dataset while leaving\n",
    "    # enough recent data for validation.\n",
    "    train, val = series.split_after(pd.Timestamp('20170901'))\n",
    "\n",
    "    # Normalize the series to aid the model training process by ensuring numerical values\n",
    "    # have a mean of 0 and standard deviation of 1. This helps improve the stability and\n",
    "    # speed of convergence during training.\n",
    "    scaler = Scaler()\n",
    "    train_transformed = scaler.fit_transform(train)\n",
    "    val_transformed = scaler.transform(val)\n",
    "\n",
    "    # Add day as a covariate:\n",
    "    # Including the day of the month as a one-hot encoded covariate to provide the model\n",
    "    # with additional contextual information about potential daily periodicities in\n",
    "    # hydroelectric generation.\n",
    "    day_series = datetime_attribute_timeseries(series, attribute='day', one_hot=True)\n",
    "    return train_transformed, val_transformed, scaler, day_series\n",
    "\n",
    "\n",
    "train_transformed, val_transformed, scaler, day_series = preprocess_train_and_val_and_save(df)\n",
    "\n",
    "# Plotting the normalized training and validation series to visualize the data\n",
    "# the model will be trained and validated on. This visualization helps in understanding\n",
    "# the general trend and seasonality in the data, providing insights into the model's\n",
    "# potential forecasting challenges.\n",
    "plt.figure(figsize=(10, 3))\n",
    "train_transformed.plot(label='Train')\n",
    "val_transformed.plot(label='Validation')\n",
    "plt.legend()\n",
    "plt.title('Normalized Training and Validation Series for Hydroelectric Generation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a5af6-c99b-4546-bd9a-457171a99a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd9f74-c031-458c-9da9-9c842a22b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scaler to use for inferencing\n",
    "# Assuming 'scaler' is your Scaler object\n",
    "scaler_file_path = 'scaler.pkl'  # Specify the path to save the scaler\n",
    "\n",
    "# Save the scaler object to a file\n",
    "with open(scaler_file_path, 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315e97f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We configure and train the TCN model using the prepared dataset.\n",
    "\n",
    "### Overview of TCN Architecture and Hyperparameters: \n",
    "The TCN model uses a series of dilated convolutional layers that allow the network to have an exponentially increasing receptive field. This means the model can incorporate information from points further back in the time series without a proportional increase in computational complexity. The architecture is particularly suited for time series data because it can effectively capture long-term dependencies and patterns. Key features include causal convolutions (ensuring predictions at time t are only dependent on data from times t' <= t) and residual connections (helping with training deep networks by allowing gradients to flow through the network's layers more effectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and train the TCN model\n",
    "model = TCNModel(\n",
    "    # input_chunk_length defines the length of the input sequences. For our model,\n",
    "    # we're looking at 365 days of data to predict the next 7 days. This parameter\n",
    "    # is crucial for capturing seasonal patterns within a year.\n",
    "    input_chunk_length=365,\n",
    "\n",
    "    # output_chunk_length defines the length of the forecasted output sequence. Here,\n",
    "    # we aim to predict energy demand for the next 7 days based on the input sequence.\n",
    "    output_chunk_length=7,\n",
    "\n",
    "    # n_epochs specifies the number of times the model will work through the entire\n",
    "    # training dataset. More epochs can lead to better learning, but also a risk of overfitting.\n",
    "    n_epochs=50,\n",
    "\n",
    "    # dropout is a regularization technique where input and recurrent connections to\n",
    "    # LSTM units are probabilistically excluded from activation and weight updates while\n",
    "    # training a network. This helps prevent overfitting. We set it to 0.2, meaning there's\n",
    "    # a 20% chance that any given connection is dropped.\n",
    "    dropout=0.2,\n",
    "\n",
    "    # dilation_base controls the spacing between the kernel points in dilated convolutions.\n",
    "    # It's part of how TCNs manage to have a large receptive field while keeping the model\n",
    "    # depth manageable. A dilation_base of 2 means the spacing between kernel points\n",
    "    # will double with each layer, helping the model to efficiently learn hierarchical\n",
    "    # representations of time series data.\n",
    "    dilation_base=2,\n",
    "    weight_norm=True,\n",
    "    save_checkpoints=True,\n",
    "    nr_epochs_val_period=1,\n",
    "    force_reset=True,\n",
    "    # kernel_size is the size of the convolutional kernel. It determines how many input\n",
    "    # values will be considered at once by each convolution operation. A size of 5 means\n",
    "    # each convolution will combine information from 5 time steps.\n",
    "    kernel_size=5,\n",
    "\n",
    "    # num_filters specifies the number of filters in the convolutional layers, which\n",
    "    # translates to the number of output channels. More filters can capture more information\n",
    "    # but increase computational complexity. We choose 8 as a balance.\n",
    "    num_filters=8,\n",
    "\n",
    "    # random_state sets the seed for random number generation, ensuring reproducibility\n",
    "    # of the model training process.\n",
    "    random_state=0,\n",
    "\n",
    "    # Additional arguments passed to the PyTorch Lightning trainer. In this case, we're\n",
    "    # specifying the training to be performed on CPU and customizing callbacks for training\n",
    "    # progress. This is handled by the generate_torch_kwargs() helper function.\n",
    "    **generate_torch_kwargs()\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    series=train_transformed,\n",
    "    past_covariates=day_series,\n",
    "    val_series=val_transformed,\n",
    "    val_past_covariates=day_series\n",
    ")\n",
    "# Save the model\n",
    "model.save('models/TCN_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf0252",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "After training, we evaluate the model's performance on the validation set using backtesting.\n",
    "\n",
    "Run backtest validation: Historical forecasting (backtesting) simulates the prediction of past data at various points in time, as if we were predicting the future. Here, we're using our trained model to forecast 7 days into the future, given past covariates, and we're not retraining the model in the process. This allows us to compare the model's forecasts to the actual observed values in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest = model.historical_forecasts(\n",
    "    series=val_transformed,\n",
    "    past_covariates=day_series,\n",
    "    forecast_horizon=7,\n",
    "    retrain=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Plotting actual vs forecasted\n",
    "plt.figure(figsize=(10, 6))\n",
    "val_transformed.plot(label='Actual')\n",
    "backtest.plot(label='Forecast')\n",
    "plt.legend()\n",
    "plt.title('Validation Set - Actual vs Forecast')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda2d28-17e5-44f2-94a7-9ca1ba040398",
   "metadata": {},
   "source": [
    "Comparing the backtest forecasts against the ground truth in the validation set to quantify the model's performance. Common metrics for this purpose include RMSE (Root Mean Square Error) and MAE (Mean Absolute Error). Lower values indicate better model performance.\n",
    "\n",
    "The **RMSE** metric provides the standard deviation of the residuals (prediction errors), indicating how far, on average, the forecasts are from the actual values. **MAE** offers a straightforward average of absolute errors. Both metrics are useful for understanding the accuracy and performance of the forecasting model in real-world terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c25724-0726-4230-98a6-01dfc30f3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import rmse, mae\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "rmse_val = rmse(val_transformed, backtest)\n",
    "mae_val = mae(val_transformed, backtest)\n",
    "\n",
    "# Display the metrics\n",
    "print(f'RMSE (Validation): {rmse_val:.2f}')\n",
    "print(f'MAE (Validation): {mae_val:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6d467",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This tutorial demonstrated the process of forecasting energy demand using a TCN model. Starting from data preparation to model training and evaluation, each step was detailed for clarity. The forecasted results on the validation set show the model's ability to predict future values, essential for decision-making in energy management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
